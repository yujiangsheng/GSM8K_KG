#!/usr/bin/env python3
"""
GSM8K Knowledge Graph Generator - å®Œæ•´æ•°æ®é›†ç‰ˆæœ¬
æ”¯æŒåˆ†æ‰¹å¤„ç†ã€æ–­ç‚¹ç»­ä¼ ã€è¿›åº¦ä¿å­˜
ä½¿ç”¨ Qwen2.5-7B-Instruct çš„æ­£ç¡® Chat æ¨¡æ¿
"""

import json
import torch
import re
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Set
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM


def get_best_device() -> str:
    """è·å–æœ€ä½³å¯ç”¨è®¾å¤‡ï¼šGPU > MPS > CPU"""
    if torch.cuda.is_available():
        print(f"âœ“ ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("âœ“ ä½¿ç”¨ MPS (Apple Silicon)")
        return "mps"
    else:
        print("âœ“ ä½¿ç”¨ CPU")
        return "cpu"


class KnowledgeGraphGenerator:
    """çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨ - ä½¿ç”¨ Qwen Chat æ¨¡æ¿"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        self.device = get_best_device()
        self.model_name = model_name
        
        print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else self.device,
            trust_remote_code=True
        )
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    def generate(self, question_id: int, question: str, ground_truth: str, max_retries: int = 3) -> Optional[Dict]:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±ï¼ŒéªŒè¯ç­”æ¡ˆï¼Œé”™è¯¯æ—¶é‡è¯•"""
        kg = None
        last_answer = None
        
        for attempt in range(max_retries):
            try:
                # æ„å»ºæ¶ˆæ¯
                messages = self._build_messages(question, ground_truth if attempt > 0 else None, last_answer)
                
                # ä½¿ç”¨ Qwen çš„ chat æ¨¡æ¿
                text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=1500,
                        temperature=0.7,
                        top_p=0.9,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                # åªå–ç”Ÿæˆçš„æ–°å†…å®¹
                generated_ids = outputs[0][inputs.input_ids.shape[1]:]
                response = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
                
                # è°ƒè¯•è¾“å‡ºï¼ˆä»…ç¬¬ä¸€æ¬¡å°è¯•ï¼‰
                if attempt == 0:
                    print(f"    [è°ƒè¯•] è¾“å‡ºå‰150å­—: {response[:150].replace(chr(10), ' ')}...")
                
                # è§£æå“åº”
                kg = self._parse_response(question_id, question, ground_truth, response)
                last_answer = kg["final_answer"]
                
                # éªŒè¯ç­”æ¡ˆ
                if self._validate_answer(kg["final_answer"], ground_truth):
                    kg["status"] = "correct"
                    kg["attempts"] = attempt + 1
                    return kg
                
                if attempt < max_retries - 1:
                    print(f"    âš ï¸ ç­”æ¡ˆ '{kg['final_answer']}' é”™è¯¯ï¼Œæ­£ç¡® '{ground_truth}'ï¼Œé‡è¯• {attempt + 2}/{max_retries}")
                    
            except Exception as e:
                print(f"    âŒ é”™è¯¯: {str(e)[:80]}")
                if kg is None:
                    kg = {
                        "question_id": question_id,
                        "question": question,
                        "ground_truth_answer": ground_truth,
                        "cot": "",
                        "solution_steps": "",
                        "final_answer": "",
                        "problem_type": "",
                        "required_knowledge": ""
                    }
        
        if kg:
            kg["status"] = "incorrect"
            kg["attempts"] = max_retries
        return kg
    
    def _build_messages(self, question: str, correct_answer: str = None, last_answer: str = None) -> list:
        """æ„å»º Qwen Chat æ¶ˆæ¯æ ¼å¼"""
        
        system_msg = """ä½ æ˜¯ä¸€ä½æ•°å­¦æ•™è‚²ä¸“å®¶ã€‚è¯·åˆ†ææ•°å­¦é—®é¢˜å¹¶ç”Ÿæˆç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€‚

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼ˆä½¿ç”¨ã€ã€‘æ ‡è®°æ¯ä¸ªéƒ¨åˆ†ï¼‰ï¼š

ã€é“¾å¼æ€ç»´ã€‘
é€æ­¥æ¨ç†è¿‡ç¨‹

ã€è§£é¢˜æ­¥éª¤ã€‘
è¯¦ç»†è®¡ç®—è¿‡ç¨‹

ã€æœ€ç»ˆç­”æ¡ˆã€‘
åªå†™ä¸€ä¸ªæ•°å­—

ã€é—®é¢˜ç±»å‹ã€‘
é—®é¢˜åˆ†ç±»

ã€æ‰€éœ€çŸ¥è¯†ã€‘
éœ€è¦çš„æ•°å­¦æ¦‚å¿µ"""

        if correct_answer and last_answer:
            user_msg = f"""é—®é¢˜ï¼š{question}

ä½ ä¸Šæ¬¡çš„ç­”æ¡ˆ {last_answer} æ˜¯é”™è¯¯çš„ã€‚æ­£ç¡®ç­”æ¡ˆæ˜¯ {correct_answer}ã€‚
è¯·é‡æ–°åˆ†æï¼Œã€æœ€ç»ˆç­”æ¡ˆã€‘å¿…é¡»æ˜¯ {correct_answer}"""
        else:
            user_msg = f"""é—®é¢˜ï¼š{question}

è¯·è§£ç­”è¿™é“æ•°å­¦é¢˜ã€‚"""

        return [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg}
        ]
    
    def _parse_response(self, qid: int, question: str, ground_truth: str, response: str) -> Dict:
        """è§£ææ¨¡å‹å“åº”ä¸ºç»“æ„åŒ–çŸ¥è¯†å›¾è°±"""
        
        def extract_section(text: str, section_name: str) -> str:
            """æå–æŒ‡å®šéƒ¨åˆ†çš„å†…å®¹"""
            pattern = rf"ã€{section_name}ã€‘\s*(.*?)(?=ã€|$)"
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return match.group(1).strip()[:500]
            return ""
        
        # æå–å„éƒ¨åˆ†
        cot = extract_section(response, "é“¾å¼æ€ç»´")
        steps = extract_section(response, "è§£é¢˜æ­¥éª¤")
        answer_text = extract_section(response, "æœ€ç»ˆç­”æ¡ˆ")
        problem_type = extract_section(response, "é—®é¢˜ç±»å‹")
        knowledge = extract_section(response, "æ‰€éœ€çŸ¥è¯†")
        
        # ä»ç­”æ¡ˆæ–‡æœ¬ä¸­æå–æ•°å­—
        numbers = re.findall(r'-?\d+(?:\.\d+)?', answer_text)
        if numbers:
            final_answer = numbers[0]
        else:
            # å°è¯•ä»æ•´ä¸ªå“åº”ä¸­æŸ¥æ‰¾ç­”æ¡ˆæ¨¡å¼
            patterns = [
                r'ç­”æ¡ˆ[æ˜¯ä¸ºï¼š:\s]+(-?\d+(?:\.\d+)?)',
                r'ç­‰äº\s*(-?\d+(?:\.\d+)?)',
                r'å…±[æœ‰æ˜¯]\s*(-?\d+(?:\.\d+)?)',
                r'=\s*(-?\d+(?:\.\d+)?)\s*(?:å…ƒ|ä¸ª|é¡µ|å²|å¤©|å°æ—¶|åˆ†é’Ÿ)?$',
            ]
            final_answer = ""
            for pat in patterns:
                match = re.search(pat, response, re.MULTILINE)
                if match:
                    final_answer = match.group(1)
                    break
            
            if not final_answer:
                # æœ€åå°è¯•ï¼šå–å“åº”ä¸­æœ€åä¸€ä¸ªæ•°å­—
                all_nums = re.findall(r'-?\d+(?:\.\d+)?', response)
                if all_nums:
                    final_answer = all_nums[-1]
        
        return {
            "question_id": qid,
            "question": question,
            "ground_truth_answer": ground_truth,
            "cot": cot,
            "solution_steps": steps,
            "final_answer": final_answer,
            "problem_type": problem_type,
            "required_knowledge": knowledge
        }
    
    def _validate_answer(self, model_answer: str, ground_truth: str) -> bool:
        """éªŒè¯ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        try:
            model_nums = re.findall(r'-?\d+(?:\.\d+)?', str(model_answer))
            truth_nums = re.findall(r'-?\d+(?:\.\d+)?', str(ground_truth))
            
            if not model_nums or not truth_nums:
                return False
            
            model_num = float(model_nums[0])
            truth_num = float(truth_nums[0])
            
            return abs(model_num - truth_num) < 0.001
        except:
            return str(model_answer).strip() == str(ground_truth).strip()


class BatchProcessor:
    """åˆ†æ‰¹å¤„ç†å™¨"""
    
    def __init__(self, output_dir: str = "output", batch_size: int = 100):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.batch_size = batch_size
        self.generator = None
        
    def load_processed_ids(self) -> Set[int]:
        """åŠ è½½å·²å¤„ç†çš„é—®é¢˜ID"""
        processed = set()
        for batch_file in self.output_dir.glob("batch_*.json"):
            with open(batch_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for kg in data.get("knowledge_graphs", []):
                    processed.add(kg["question_id"])
        return processed
    
    def save_batch(self, batch_num: int, knowledge_graphs: list, stats: dict):
        """ä¿å­˜æ‰¹æ¬¡ç»“æœ"""
        batch_file = self.output_dir / f"batch_{batch_num:04d}.json"
        result = {
            "batch_num": batch_num,
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "knowledge_graphs": knowledge_graphs
        }
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ {batch_num} -> {batch_file.name}")
    
    def merge_all_batches(self, output_file: str = "GSM8K_KG.json"):
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡"""
        print("\nğŸ“¦ åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡...")
        all_kgs = []
        batch_files = sorted(self.output_dir.glob("batch_*.json"))
        
        for batch_file in batch_files:
            with open(batch_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_kgs.extend(data.get("knowledge_graphs", []))
        
        all_kgs.sort(key=lambda x: x["question_id"])
        
        correct = sum(1 for kg in all_kgs if kg["status"] == "correct")
        result = {
            "metadata": {
                "dataset": "GSM8K",
                "model": "Qwen2.5-7B-Instruct",
                "total": len(all_kgs),
                "correct": correct,
                "incorrect": len(all_kgs) - correct,
                "accuracy": f"{correct/len(all_kgs)*100:.2f}%" if all_kgs else "0%",
                "generated_at": datetime.now().isoformat()
            },
            "knowledge_graphs": all_kgs
        }
        
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆå¹¶å®Œæˆ: {output_path}")
        if all_kgs:
            print(f"ğŸ“Š æ€»è®¡: {len(all_kgs)} | æ­£ç¡®: {correct} | å‡†ç¡®ç‡: {correct/len(all_kgs)*100:.2f}%")
        return output_path
    
    def process_dataset(self, start_batch: int = 0, max_batches: int = None):
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
        print("=" * 70)
        print("GSM8K çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨ - Qwen2.5-7B-Instruct")
        print("=" * 70)
        
        self.generator = KnowledgeGraphGenerator()
        
        print("ğŸ“¥ åŠ è½½ GSM8K æ•°æ®é›†...")
        dataset = load_dataset("openai/gsm8k", "main", split="train")
        total_problems = len(dataset)
        print(f"âœ“ æ•°æ®é›†å¤§å°: {total_problems} ä¸ªé—®é¢˜")
        
        processed_ids = self.load_processed_ids()
        print(f"âœ“ å·²å¤„ç†: {len(processed_ids)} ä¸ªé—®é¢˜")
        
        total_batches = (total_problems + self.batch_size - 1) // self.batch_size
        if max_batches:
            total_batches = min(total_batches, start_batch + max_batches)
        
        print(f"âœ“ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"âœ“ æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        print()
        
        start_time = time.time()
        
        for batch_num in range(start_batch, total_batches):
            batch_start = batch_num * self.batch_size
            batch_end = min(batch_start + self.batch_size, total_problems)
            
            print(f"\n{'='*70}")
            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_num + 1}/{total_batches} (é—®é¢˜ {batch_start}-{batch_end-1})")
            print(f"{'='*70}")
            
            batch_kgs = []
            batch_correct = 0
            batch_skipped = 0
            
            for idx in range(batch_start, batch_end):
                if idx in processed_ids:
                    batch_skipped += 1
                    continue
                
                item = dataset[idx]
                match = re.search(r'####\s*(-?\d+(?:\.\d+)?)', item['answer'])
                ground_truth = match.group(1) if match else item['answer'].split('\n')[-1].strip()
                
                progress = idx - batch_start + 1
                total_in_batch = batch_end - batch_start
                print(f"  [{progress}/{total_in_batch}] ID={idx}: {item['question'][:45]}...")
                
                kg = self.generator.generate(
                    question_id=idx,
                    question=item["question"],
                    ground_truth=ground_truth,
                    max_retries=3
                )
                
                if kg:
                    batch_kgs.append(kg)
                    if kg["status"] == "correct":
                        batch_correct += 1
                        print(f"    âœ… æ­£ç¡® (å°è¯• {kg['attempts']} æ¬¡)")
                    else:
                        print(f"    âŒ é”™è¯¯ (æ¨¡å‹: {kg['final_answer']}, æ­£ç¡®: {ground_truth})")
            
            if batch_kgs:
                stats = {
                    "processed": len(batch_kgs),
                    "skipped": batch_skipped,
                    "correct": batch_correct,
                    "incorrect": len(batch_kgs) - batch_correct
                }
                self.save_batch(batch_num, batch_kgs, stats)
            
            elapsed = time.time() - start_time
            completed_batches = batch_num - start_batch + 1
            avg_time = elapsed / completed_batches
            remaining = (total_batches - batch_num - 1) * avg_time
            
            print(f"\n  ğŸ“Š æ‰¹æ¬¡: å¤„ç† {len(batch_kgs)} | æ­£ç¡® {batch_correct} | è·³è¿‡ {batch_skipped}")
            print(f"  â±ï¸  å·²ç”¨: {elapsed/60:.1f}åˆ†é’Ÿ | å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ")
        
        self.merge_all_batches()
        print(f"\nğŸ‰ å®Œæˆï¼æ€»ç”¨æ—¶: {(time.time()-start_time)/3600:.2f}å°æ—¶")


def main():
    parser = argparse.ArgumentParser(description="GSM8K çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨")
    parser.add_argument("--batch-size", type=int, default=100, help="æ¯æ‰¹å¤„ç†çš„é—®é¢˜æ•°é‡")
    parser.add_argument("--start-batch", type=int, default=0, help="å¼€å§‹çš„æ‰¹æ¬¡ç¼–å·")
    parser.add_argument("--max-batches", type=int, default=None, help="æœ€å¤§å¤„ç†æ‰¹æ¬¡æ•°")
    parser.add_argument("--output-dir", type=str, default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--merge-only", action="store_true", help="ä»…åˆå¹¶å·²æœ‰æ‰¹æ¬¡")
    
    args = parser.parse_args()
    
    processor = BatchProcessor(output_dir=args.output_dir, batch_size=args.batch_size)
    
    if args.merge_only:
        processor.merge_all_batches()
    else:
        processor.process_dataset(start_batch=args.start_batch, max_batches=args.max_batches)


if __name__ == "__main__":
    main()
